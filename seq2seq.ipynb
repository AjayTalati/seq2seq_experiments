{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "from tensorflow.models.rnn import seq2seq\n",
    "from tensorflow.models.rnn.translate import data_utils\n",
    "\n",
    "\n",
    "class DateNormModel(object):\n",
    "   \n",
    "    def __init__(self, num_source_labels, num_target_labels, hidden_units,\n",
    "                 num_layers, max_gradient_norm, batch_size, learning_rate,\n",
    "                 learning_rate_decay_factor, use_lstm=False, forward_only=False):\n",
    "        \"\"\"Create the model.\n",
    "        Args:\n",
    "          num_source_labels: size of the source vocabulary.\n",
    "          num_target_labels: size of the target vocabulary.\n",
    "          hidden_units: number of units in each layer of the model.\n",
    "          num_layers: number of layers in the model.\n",
    "          max_gradient_norm: gradients will be clipped to maximally this norm.\n",
    "          batch_size: the size of the batches used during training;\n",
    "            the model construction is independent of batch_size, so it can be\n",
    "            changed after initialization if this is convenient, e.g., for decoding.\n",
    "          learning_rate: learning rate to start with.\n",
    "          learning_rate_decay_factor: decay learning rate by this much when needed.\n",
    "          use_lstm: if true, we use LSTM cells instead of GRU cells.\n",
    "          num_samples: number of samples for sampled softmax.\n",
    "          forward_only: if set, we do not construct the backward pass in the model.\n",
    "        \"\"\"\n",
    "        self.source_vocab_size = source_vocab_size\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.buckets = buckets\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = tf.Variable(float(learning_rate), trainable=False)\n",
    "        self.learning_rate_decay_op = self.learning_rate.assign(\n",
    "            self.learning_rate * learning_rate_decay_factor)\n",
    "        self.global_step = tf.Variable(0, trainable=False)\n",
    "        \n",
    "        \n",
    "        single_cell = rnn_cell.BasicLSTMCell(size)\n",
    "        if num_layers > 1:\n",
    "            cell = rnn_cell.MultiRNNCell([single_cell] * num_layers)\n",
    "            \n",
    "        # The seq2seq function: we use embedding for the input and attention.\n",
    "        def seq2seq_f(encoder_inputs, decoder_inputs, do_decode):\n",
    "            return seq2seq.basic_rnn_seq2seq(encoder_inputs, decoder_inputs, cell)\n",
    "        \n",
    "        # Feeds for inputs.\n",
    "        self.encoder_inputs = []\n",
    "        self.decoder_inputs = []\n",
    "        self.target_weights = []\n",
    "        for i in xrange(buckets[-1][0]):  # Last bucket is the biggest one.\n",
    "            self.encoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                      name=\"encoder{0}\".format(i)))\n",
    "        for i in xrange(buckets[-1][1] + 1):\n",
    "            self.decoder_inputs.append(tf.placeholder(tf.int32, shape=[None],\n",
    "                                                      name=\"decoder{0}\".format(i)))\n",
    "            \n",
    "        # Our targets are decoder inputs shifted by one.\n",
    "        targets = [self.decoder_inputs[i + 1]\n",
    "                   for i in xrange(len(self.decoder_inputs) - 1)]\n",
    "        \n",
    "        # Training outputs and losses.\n",
    "        if forward_only:\n",
    "            self.outputs, self.losses = seq2seq.model_with_buckets(\n",
    "                self.encoder_inputs, self.decoder_inputs, targets,\n",
    "                self.target_weights, buckets, self.target_vocab_size,\n",
    "                lambda x, y: seq2seq_f(x, y, True),\n",
    "                softmax_loss_function=softmax_loss_function)\n",
    "            # If we use output projection, we need to project outputs for decoding.\n",
    "            if output_projection is not None:\n",
    "                for b in xrange(len(buckets)):\n",
    "                    self.outputs[b] = [tf.matmul(output, output_projection[0]) +\n",
    "                                       output_projection[1]\n",
    "                                       for output in self.outputs[b]]\n",
    "        else:\n",
    "            self.outputs, self.losses = seq2seq.model_with_buckets(\n",
    "                self.encoder_inputs, self.decoder_inputs, targets,\n",
    "                self.target_weights, buckets, self.target_vocab_size,\n",
    "                lambda x, y: seq2seq_f(x, y, False),\n",
    "                softmax_loss_function=softmax_loss_function)\n",
    "        # Gradients and SGD update operation for training the model.\n",
    "        params = tf.trainable_variables()\n",
    "        if not forward_only:\n",
    "            self.gradient_norms = []\n",
    "            self.updates = []\n",
    "            opt = tf.train.GradientDescentOptimizer(self.learning_rate)\n",
    "            for b in xrange(len(buckets)):\n",
    "                gradients = tf.gradients(self.losses[b], params)\n",
    "                clipped_gradients, norm = tf.clip_by_global_norm(gradients,\n",
    "                                                                 max_gradient_norm)\n",
    "                self.gradient_norms.append(norm)\n",
    "                self.updates.append(opt.apply_gradients(\n",
    "                    zip(clipped_gradients, params), global_step=self.global_step))\n",
    "        self.saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "    def step(self, session, encoder_inputs, decoder_inputs, target_weights,\n",
    "             bucket_id, forward_only):\n",
    "        \"\"\"Run a step of the model feeding the given inputs.\n",
    "        Args:\n",
    "          session: tensorflow session to use.\n",
    "          encoder_inputs: list of numpy int vectors to feed as encoder inputs.\n",
    "          decoder_inputs: list of numpy int vectors to feed as decoder inputs.\n",
    "          target_weights: list of numpy float vectors to feed as target weights.\n",
    "          bucket_id: which bucket of the model to use.\n",
    "          forward_only: whether to do the backward step or only forward.\n",
    "        Returns:\n",
    "          A triple consisting of gradient norm (or None if we did not do backward),\n",
    "          average perplexity, and the outputs.\n",
    "        Raises:\n",
    "          ValueError: if length of encoder_inputs, decoder_inputs, or\n",
    "            target_weights disagrees with bucket size for the specified bucket_id.\n",
    "        \"\"\"\n",
    "        # Check if the sizes match.\n",
    "        encoder_size, decoder_size = self.buckets[bucket_id]\n",
    "        if len(encoder_inputs) != encoder_size:\n",
    "            raise ValueError(\"Encoder length must be equal to the one in bucket,\"\n",
    "                             \" %d != %d.\" % (len(encoder_inputs), encoder_size))\n",
    "        if len(decoder_inputs) != decoder_size:\n",
    "            raise ValueError(\"Decoder length must be equal to the one in bucket,\"\n",
    "                             \" %d != %d.\" % (len(decoder_inputs), decoder_size))\n",
    "        if len(target_weights) != decoder_size:\n",
    "            raise ValueError(\"Weights length must be equal to the one in bucket,\"\n",
    "                             \" %d != %d.\" % (len(target_weights), decoder_size))\n",
    "        # Input feed: encoder inputs, decoder inputs, target_weights, as provided.\n",
    "        input_feed = {}\n",
    "        for l in xrange(encoder_size):\n",
    "            input_feed[self.encoder_inputs[l].name] = encoder_inputs[l]\n",
    "        for l in xrange(decoder_size):\n",
    "            input_feed[self.decoder_inputs[l].name] = decoder_inputs[l]\n",
    "            input_feed[self.target_weights[l].name] = target_weights[l]\n",
    "        # Since our targets are decoder inputs shifted by one, we need one more.\n",
    "        last_target = self.decoder_inputs[decoder_size].name\n",
    "        input_feed[last_target] = np.zeros([self.batch_size], dtype=np.int32)\n",
    "        # Output feed: depends on whether we do a backward step or not.\n",
    "        if not forward_only:\n",
    "            output_feed = [self.updates[bucket_id],  # Update Op that does SGD.\n",
    "                           self.gradient_norms[bucket_id],  # Gradient norm.\n",
    "                           self.losses[bucket_id]]  # Loss for this batch.\n",
    "        else:\n",
    "            output_feed = [self.losses[bucket_id]]  # Loss for this batch.\n",
    "            for l in xrange(decoder_size):  # Output logits.\n",
    "                output_feed.append(self.outputs[bucket_id][l])\n",
    "        outputs = session.run(output_feed, input_feed)\n",
    "        if not forward_only:\n",
    "            return outputs[1], outputs[2], None  # Gradient norm, loss, no outputs.\n",
    "        else:\n",
    "            return None, outputs[0], outputs[1:]  # No gradient norm, loss, outputs.\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "raw_data = reader.ptb_raw_data(FLAGS.data_path)\n",
    "train_data, valid_data, test_data, _ = raw_data\n",
    "\n",
    "for step, (x, y) in enumerate(reader.ptb_iterator(train_data,1 3)):\n",
    "    print(step, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'February 21, 2016'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%d-%A-%B-%b-%a\n",
      "03-Saturday-August-Aug-Sat -> 2047-08-03\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import random\n",
    "\n",
    "from random import randrange\n",
    "from datetime import timedelta\n",
    "\n",
    "START_DATE = datetime.strptime('1950-01-01', '%Y-%m-%d')\n",
    "END_DATE = datetime.strptime('2050-12-31', '%Y-%m-%d')\n",
    "FORMAT_TOKENS = ('%a', '%A', '%d', '%b', '%B', '%m', '%y', '%Y')\n",
    "\n",
    "def random_date(start=START_DATE, end=END_DATE):\n",
    "    \"\"\"\n",
    "    This function will return a random datetime between two datetime \n",
    "    objects.\n",
    "    \"\"\"\n",
    "    delta = end - start\n",
    "    int_delta = (delta.days * 24 * 60 * 60) + delta.seconds\n",
    "    random_second = randrange(int_delta)\n",
    "    return start + timedelta(seconds=random_second)\n",
    "\n",
    "def random_date_string(date):\n",
    "    random_format_tokens = random.sample(FORMAT_TOKENS, 5)\n",
    "    date_format = '-'.join(random_format_tokens)\n",
    "    print(date_format)\n",
    "    return date.strftime(date_format)\n",
    "    #print(\"\\{\\:{}\\}\".format(date_format))\n",
    "\n",
    "date = random_date()\n",
    "date_string = random_date_string(date)\n",
    "    \n",
    "print(date_string, \"->\", date.strftime('%Y-%m-%d')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31/32 (epoch 0), train_loss = 1.858, time/batch = 0.023\n",
      "31/32 (epoch 1), train_loss = 1.594, time/batch = 0.023\n",
      "31/32 (epoch 2), train_loss = 1.262, time/batch = 0.022\n",
      "31/32 (epoch 3), train_loss = 1.159, time/batch = 0.023\n",
      "31/32 (epoch 4), train_loss = 1.089, time/batch = 0.039\n",
      "31/32 (epoch 5), train_loss = 1.147, time/batch = 0.023\n",
      "31/32 (epoch 6), train_loss = 1.131, time/batch = 0.024\n",
      "31/32 (epoch 7), train_loss = 1.006, time/batch = 0.024\n",
      "31/32 (epoch 8), train_loss = 0.937, time/batch = 0.023\n",
      "31/32 (epoch 9), train_loss = 0.908, time/batch = 0.023\n",
      "encoder_inputs (8, 8, 43)\n",
      "[[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]]\n",
      "1 (8, 11)\n",
      "['GO19830831', 'GO19800823', 'GO19890913', 'GO19780113', 'GO20410322', 'GO20391024', 'GO20420821', 'GO19841003']\n",
      "['36669906']\n",
      "[('GO19830831', '36669906')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from date_norm_model import DateNormModel, train\n",
    "from date_generator import INPUT_SYMBOL_TO_IDX, INPUT_SEQ_LEN, OUTPUT_SYMBOL_TO_IDX, OUTPUT_SEQ_LEN\n",
    "from date_generator import generate_training_data, OUTPUT_SYMBOLS\n",
    "from date_norm_model import decode_output_sequences\n",
    "    \n",
    "hidden_units = 128\n",
    "batch_size = 8\n",
    "\n",
    "with tf.Graph().as_default(), tf.Session() as session:\n",
    "    \n",
    "    with tf.variable_scope(\"model\", reuse=None):\n",
    "        training_model = DateNormModel(session=session,\n",
    "                                        hidden_units = hidden_units, \n",
    "                                        input_sequence_len = INPUT_SEQ_LEN,\n",
    "                                        output_sequence_len = OUTPUT_SEQ_LEN,\n",
    "                                        num_input_symbols = len(INPUT_SYMBOL_TO_IDX),\n",
    "                                        num_output_symbols = len(OUTPUT_SYMBOL_TO_IDX),\n",
    "                                        batch_size = batch_size,\n",
    "                                        is_training=True)\n",
    "        \n",
    "    with tf.variable_scope(\"model\", reuse=True):\n",
    "        testing_model = DateNormModel(session=session,\n",
    "                                        hidden_units = hidden_units, \n",
    "                                        input_sequence_len = INPUT_SEQ_LEN,\n",
    "                                        output_sequence_len = OUTPUT_SEQ_LEN,\n",
    "                                        num_input_symbols = len(INPUT_SYMBOL_TO_IDX),\n",
    "                                        num_output_symbols = len(OUTPUT_SYMBOL_TO_IDX),\n",
    "                                        batch_size = batch_size,\n",
    "                                        is_training=False)\n",
    "        \n",
    "    \n",
    "    train(model=training_model, \n",
    "          batch_size=batch_size,\n",
    "          num_epochs=10,\n",
    "          batches_per_epoch=32)\n",
    "    \n",
    "\n",
    "    data_iterator = generate_training_data(batch_size=batch_size)\n",
    "\n",
    "    x, y = next(data_iterator)\n",
    "    #print(batch_size, len(x), len(INPUT_SYMBOL_TO_IDX))\n",
    "    target_strings = decode_output_sequences(y, symbols=OUTPUT_SYMBOLS)\n",
    "    symbol_probs = testing_model.predict(x)\n",
    "    pred_strings = decode_output_sequences(symbol_probs, symbols=OUTPUT_SYMBOLS)\n",
    "    print(target_strings)\n",
    "    print(pred_strings)\n",
    "    print(list(zip(target_strings, pred_strings)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-0f5b473d99c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mtarget_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_output_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOUTPUT_SYMBOLS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msymbol_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtesting_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mpred_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecode_output_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msymbol_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msymbols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOUTPUT_SYMBOLS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_strings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_strings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/erik/proj/pycon/seq2seq/date_norm_model.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, encoder_inputs)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0minput_feed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0msymbol_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_probs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_feed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msymbol_probs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/erik/anaconda/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mdoesn\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0mt\u001b[0m \u001b[0mexist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \"\"\"\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mpartial_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/erik/anaconda/lib/python3.4/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;31m# Check session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "iterator = generate_training_data(batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8, 43) (1, 9, 11)\n",
      "[[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.]\n",
      "  [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.\n",
      "    0.  0.  0.  0.  0.  0.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iterator)\n",
    "print(x.shape, y.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['41110402']\n",
      "['GO20401114']\n"
     ]
    }
   ],
   "source": [
    "from date_generator import generate_training_data, OUTPUT_SYMBOLS, INPUT_SYMBOLS\n",
    "input_strings = decode_output_sequences(x, symbols=INPUT_SYMBOLS)\n",
    "print(input_strings)\n",
    "target_strings = decode_output_sequences(y, symbols=OUTPUT_SYMBOLS)\n",
    "print(target_strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.,  0.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.]],\n",
       "\n",
       "       [[ 0.,  1.,  0.,  0.],\n",
       "        [ 0.,  1.,  0.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.],\n",
       "        [ 0.,  0.,  1.,  0.]]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_to_one_hot(np.array([[0, 1, 2, 2], \n",
    "                           [1, 1, 2, 2]]), num_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  0.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 0.,  0.,  1.],\n",
       "       [ 0.,  0.,  1.]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_to_one_hot(np.array([1, 1, 2, 2]), num_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(['asd', 'asdsd'], key=len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y, x = next(generate_training_data())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " ..., \n",
      " [[ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  1. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]] [[[ 0.  0.  1. ...,  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  1. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  [ 0.  1.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  1.  0.]]\n",
      "\n",
      " [[ 0.  0.  1. ...,  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  [ 0.  0.  1. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  1. ...,  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  1.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  1.  0.]]\n",
      "\n",
      " ..., \n",
      " [[ 0.  0.  1. ...,  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  [ 0.  0.  1. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  0.  1. ...,  0.  0.  0.]\n",
      "  [ 1.  0.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  [ 0.  1.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]]\n",
      "\n",
      " [[ 0.  1.  0. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  1.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  0.  0.]\n",
      "  ..., \n",
      "  [ 0.  0.  0. ...,  0.  0.  1.]\n",
      "  [ 0.  0.  1. ...,  0.  0.  0.]\n",
      "  [ 0.  0.  0. ...,  0.  1.  0.]]]\n"
     ]
    }
   ],
   "source": [
    "print(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
